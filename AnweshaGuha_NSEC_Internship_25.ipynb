{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hFBRMBORI2p"
      },
      "source": [
        "# LangGraph + Tree-of-Thought + RAG — Internship Project\n"
      ],
      "id": "2hFBRMBORI2p"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Do7SvrRI2u"
      },
      "source": [
        "# Step 2 — Sample Data & Chunking Strategies (fixed sliding + semantic)\n",
        "sample_text = \"\"\"Artificial Intelligence (AI) is transforming industries through automation and insight.\n",
        "From recommendation systems that suggest content to autonomous vehicles making decisions, AI is everywhere.\n",
        "Retrieval-Augmented Generation (RAG) improves a language model's answers by retrieving external context\n",
        "before generating responses. Chunking breaks large documents into smaller pieces so retrieval can find\n",
        "relevant passages efficiently.\n",
        "\n",
        "In this demo we implement two chunkers:\n",
        "1) Fixed-size sliding window (with overlap)\n",
        "2) Semantic-style chunking (by sentences/paragraphs)\n",
        "\"\"\"\n",
        "\n",
        "def fixed_sliding_chunks(text, max_chars=200, overlap=50):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(len(text), start + max_chars)\n",
        "        chunks.append(text[start:end].strip())\n",
        "        start = end - overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "    return [c for c in chunks if c]\n",
        "\n",
        "def semantic_chunks(text, max_sent_per_chunk=3):\n",
        "    # very lightweight: split into sentences and group a few sentences per chunk\n",
        "    sents = [s.strip() for s in text.replace(\"\\n\", \" \").split('.') if s.strip()]\n",
        "    chunks = []\n",
        "    cur = []\n",
        "    for s in sents:\n",
        "        cur.append(s + '.')\n",
        "        if len(cur) >= max_sent_per_chunk:\n",
        "            chunks.append(' '.join(cur).strip())\n",
        "            cur = []\n",
        "    if cur:\n",
        "        chunks.append(' '.join(cur).strip())\n",
        "    return chunks\n",
        "\n",
        "# create chunks\n",
        "fixed = fixed_sliding_chunks(sample_text)\n",
        "semantic = semantic_chunks(sample_text)\n",
        "\n",
        "print('Fixed chunks count:', len(fixed))\n",
        "for i,c in enumerate(fixed,1):\n",
        "    print(f'[{i}]', c[:200])\n",
        "print('\\nSemantic chunks count:', len(semantic))\n",
        "for i,c in enumerate(semantic,1):\n",
        "    print(f'[{i}]', c[:200])"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "E_Do7SvrRI2u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nswfOD0RI2v"
      },
      "source": [
        "# Step 3 — Simple Retriever Agent (token-count vectors + cosine similarity)\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    # very small tokenizer: lowercase + split on non-alphanum\n",
        "    import re\n",
        "    tokens = re.findall(r\"\\w+\", text.lower())\n",
        "    return tokens\n",
        "\n",
        "def tf_vector(text, vocabulary=None):\n",
        "    toks = tokenize(text)\n",
        "    c = Counter(toks)\n",
        "    if vocabulary is None:\n",
        "        return c\n",
        "    # return vector over given vocabulary\n",
        "    return [c.get(w,0) for w in vocabulary]\n",
        "\n",
        "def build_corpus_vectors(documents):\n",
        "    # documents: list of dicts with 'id' and 'text'\n",
        "    # build vocabulary (top words) to keep vectors small\n",
        "    all_tokens = []\n",
        "    for d in documents:\n",
        "        all_tokens += tokenize(d['text'])\n",
        "    vocab = sorted(list(set(all_tokens)))[:500]  # cap vocab to 500 tokens\n",
        "    vectors = {}\n",
        "    for d in documents:\n",
        "        vectors[d['id']] = tf_vector(d['text'], vocabulary=vocab)\n",
        "    return vocab, vectors\n",
        "\n",
        "def cosine_sim(vec1, vec2):\n",
        "    # both are lists\n",
        "    dot = sum(a*b for a,b in zip(vec1,vec2))\n",
        "    norm1 = math.sqrt(sum(a*a for a in vec1))\n",
        "    norm2 = math.sqrt(sum(b*b for b in vec2))\n",
        "    if norm1==0 or norm2==0:\n",
        "        return 0.0\n",
        "    return dot/(norm1*norm2)\n",
        "\n",
        "# Build a small index combining fixed+semantic chunks\n",
        "documents = []\n",
        "for i,c in enumerate(fixed):\n",
        "    documents.append({'id':f'fixed_{i}','text':c,'meta':{'type':'fixed'}})\n",
        "for i,c in enumerate(semantic):\n",
        "    documents.append({'id':f'sem_{i}','text':c,'meta':{'type':'semantic'}})\n",
        "\n",
        "vocab, vectors = build_corpus_vectors(documents)\n",
        "\n",
        "def retrieve(query, top_k=3):\n",
        "    qvec = tf_vector(query, vocabulary=vocab)\n",
        "    scores = []\n",
        "    for d in documents:\n",
        "        score = cosine_sim(qvec, vectors[d['id']])\n",
        "        scores.append((score, d))\n",
        "    scores.sort(reverse=True, key=lambda x:x[0])\n",
        "    return scores[:top_k]\n",
        "\n",
        "# quick test\n",
        "print('Retrieve test for query: \"What is Retrieval-Augmented Generation?\"')\n",
        "res = retrieve('What is Retrieval-Augmented Generation?', top_k=4)\n",
        "for score, d in res:\n",
        "    print('score', round(score,3), d['id'], d['meta']['type'], d['text'][:200])"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "5nswfOD0RI2v"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnaYuZc_RI2w"
      },
      "source": [
        "# Step 4 — Query Planner Agent (decides vector vs web)\n",
        "def plan_query(query):\n",
        "    ql = query.lower()\n",
        "    use_web = any(tok in ql for tok in ['latest','today','2025','2024','news','price','score','who is','when'])\n",
        "    # naive decomposition: split on 'and' or ';' for demo\n",
        "    subqueries = [s.strip() for s in re.split(r'[;\\n]', query) if s.strip()]\n",
        "    return {'subqueries': subqueries, 'use_web': use_web}\n",
        "\n",
        "# Demo:\n",
        "import re\n",
        "print('Planner demo:')\n",
        "print(plan_query('Benchmark the in-house DL framework vs top 3 market leaders'))\n",
        "print(plan_query('What is Retrieval-Augmented Generation?'))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "xnaYuZc_RI2w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2wumqioRI2w"
      },
      "source": [
        "# Step 5 — Synthesizer + Writer Agent\n",
        "def synthesize_evidence(retrieved):\n",
        "    # retrieved: list of (score, doc)\n",
        "    # group and produce concise evidence text\n",
        "    pieces = []\n",
        "    for score, d in retrieved:\n",
        "        pieces.append({'id':d['id'], 'score':score, 'text':d['text'], 'meta':d['meta']})\n",
        "    return pieces\n",
        "\n",
        "def compute_confidence(pieces):\n",
        "    if not pieces:\n",
        "        return 0.0\n",
        "    avg = sum(p['score'] for p in pieces)/len(pieces)\n",
        "    # map to 0-1\n",
        "    return max(0.0, min(1.0, avg))\n",
        "\n",
        "def writer_generate(query, retrieved):\n",
        "    pieces = synthesize_evidence(retrieved)\n",
        "    conf = compute_confidence(pieces)\n",
        "    # simple summary: join top piece texts\n",
        "    summary = ' '.join(p['text'] for p in pieces[:2])\n",
        "    citations = [p['id'] for p in pieces]\n",
        "    return {'answer': summary, 'citations': citations, 'confidence': round(conf,3), 'pieces': pieces}\n",
        "\n",
        "# Demo writer:\n",
        "q = 'What is Retrieval-Augmented Generation?'\n",
        "res = retrieve(q, top_k=3)\n",
        "out = writer_generate(q, res)\n",
        "print('Answer:', out['answer'][:300])\n",
        "print('Citations:', out['citations'], 'Confidence:', out['confidence'])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "a2wumqioRI2w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHGkJYvGRI2x"
      },
      "source": [
        "# Step 6 — Tree of Thought (simulated): generate multiple reasoning paths and score them\n",
        "def propose_expansions(query, n=3):\n",
        "    # create simple candidate claims by splitting query into n paraphrases (mock)\n",
        "    parts = [query]  # baseline: same query\n",
        "    # lightweight variations:\n",
        "    parts += [query + ' definition', query + ' explanation']\n",
        "    return parts[:n]\n",
        "\n",
        "def evaluate_path(path_claim, retriever_func):\n",
        "    # evaluate by retrieving for the claim and using top score as path score\n",
        "    res = retriever_func(path_claim, top_k=3)\n",
        "    if not res:\n",
        "        return 0.0, res\n",
        "    return res[0][0], res  # return top score\n",
        "\n",
        "def tree_of_thought(query, beam=3, depth=2):\n",
        "    # root\n",
        "    nodes = [{'claims':[], 'score':0.0, 'retrieved':[]}]\n",
        "    for d in range(depth):\n",
        "        candidates = []\n",
        "        for node in nodes:\n",
        "            expansions = propose_expansions(query, n=beam)\n",
        "            for e in expansions:\n",
        "                score, retrieved = evaluate_path(e, retrieve)\n",
        "                new = {'claims': node['claims'] + [e], 'score': node['score'] + score, 'retrieved': retrieved}\n",
        "                candidates.append(new)\n",
        "        candidates.sort(key=lambda x: x['score'], reverse=True)\n",
        "        nodes = candidates[:beam]\n",
        "    best = nodes[0]\n",
        "    return best\n",
        "\n",
        "# Demo ToT:\n",
        "best = tree_of_thought('What is Retrieval-Augmented Generation?', beam=3, depth=2)\n",
        "print('Best path score:', best['score'])\n",
        "print('Path claims:', best['claims'])\n",
        "print('Top retrieved for final claim:')\n",
        "for s,d in best['retrieved']:\n",
        "    print(round(s,3), d['id'], d['text'][:200])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "GHGkJYvGRI2x"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z--bNqRRI2x"
      },
      "source": [
        "# Step 7 — Reviewer Agent & RAGAS-like metrics (simple)\n",
        "def reviewer_assess(answer_obj, gold_text=None):\n",
        "    # answer_obj from writer_generate\n",
        "    # simple faithfulness: check overlap of words between answer and supporting pieces\n",
        "    ans_words = set(tokenize(answer_obj['answer']))\n",
        "    support_words = set()\n",
        "    for p in answer_obj['pieces']:\n",
        "        support_words |= set(tokenize(p['text']))\n",
        "    overlap = len(ans_words & support_words)\n",
        "    faithfulness = overlap / (len(ans_words) + 1e-6)\n",
        "    recall = None\n",
        "    if gold_text:\n",
        "        gold_words = set(tokenize(gold_text))\n",
        "        recall = len(support_words & gold_words) / (len(gold_words) + 1e-6)\n",
        "    return {'faithfulness':round(faithfulness,3), 'recall': round(recall,3) if recall is not None else None}\n",
        "\n",
        "# Demo reviewer:\n",
        "gold = 'RAG is a technique that retrieves context to improve generation quality.'\n",
        "print(reviewer_assess(out, gold))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "8z--bNqRRI2x"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0f2LSqQRI2x"
      },
      "source": [
        "# Step 8 — Example Queries & Final Checklist\n",
        "queries = [\n",
        "    'What is Retrieval-Augmented Generation?',\n",
        "    'How does chunking affect retrieval?',\n",
        "    'Benchmark the in-house DL framework against top 3 leaders'\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print('\\n=== QUERY:', q)\n",
        "    plan = plan_query(q)\n",
        "    print('Plan:', plan)\n",
        "    if plan['use_web']:\n",
        "        print('Planner decided to use web retrieval (mock) — using vector retrieval as fallback.')\n",
        "    res = retrieve(q, top_k=4)\n",
        "    out = writer_generate(q, res)\n",
        "    print('Answer (snippet):', out['answer'][:200])\n",
        "    print('Citations:', out['citations'], 'Confidence:', out['confidence'])\n",
        "    print('Reviewer:', reviewer_assess(out))\n",
        "\n",
        "print('\\n--- Submission checklist ---')\n",
        "print('1) Notebook named Anwesha_Guha_Internship_25.ipynb')\n",
        "print('2) Contains chunking, retrieval, ToT demo, and reviewer')\n",
        "print('3) Ready to upload to Colab and run on CPU-only runtime')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "q0f2LSqQRI2x"
    }
  ]
}